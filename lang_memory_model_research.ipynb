{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "import random\n",
    "import os\n",
    "import deepdish as dd\n",
    "from collections import defaultdict\n",
    "#SCIPY\n",
    "from scipy.optimize import fsolve\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Zipf_Mandelbrot_CDF(n, alpha, beta=2.7):\n",
    "    \"\"\"Computes Zipf-Mandelbrot cumulative distribution function\"\"\"\n",
    "    x = np.arange(1, n + 1) + beta\n",
    "    mandelbrot_law = np.power(x, -alpha)\n",
    "    zeta = np.cumsum(mandelbrot_law )\n",
    "    return zeta / zeta[-1]\n",
    "\n",
    "def Zipf_CDF(n, alpha):\n",
    "    \"\"\"Computes Zipf cumulative distribution function.\n",
    "    Each percentage corresponds to word index in array\"\"\"\n",
    "    zipf_law = np.power(np.arange(1, n + 1), -alpha)\n",
    "    zeta = np.cumsum(zipf_law)\n",
    "    return zeta / zeta[-1]\n",
    "\n",
    "def Zipf_CDF_compressed(n, alpha, n_red=1000):\n",
    "    \"\"\"Computes Zipf cumulative distribution function.\n",
    "    It compresses real long interval n into a smaller n_red\n",
    "    by conserving relative percentages by intervals\n",
    "    Each percentage corresponds to word index in array\"\"\"\n",
    "\n",
    "    zipf_law = np.power(np.arange(1, n + 1), -alpha)\n",
    "\n",
    "    zeta = np.cumsum(zipf_law)\n",
    "    zeta = zeta / zeta[-1]\n",
    "    interv_div = np.linspace(1, n, n_red + 1).astype(np.int64)\n",
    "\n",
    "    # zeta = np.array([zeta[i1-1] for i1 in interv_div[1:]])\n",
    "    zeta = zeta[interv_div[1:] - 1]\n",
    "    return zeta\n",
    "\n",
    "def Zipf_Mand_CDF_compressed(n, alpha, beta=2.7, n_red=1000):\n",
    "    \"\"\"Computes Zipf cumulative distribution function.\n",
    "    It compresses real long interval n into a smaller n_red\n",
    "    by conserving relative percentages by intervals\n",
    "    Each percentage corresponds to word index in array\"\"\"\n",
    "    \n",
    "    x = np.arange(1, n + 1) + beta\n",
    "    zipf_mand_law = np.power(x, -alpha)\n",
    "\n",
    "    zeta = np.cumsum(zipf_mand_law)\n",
    "    zeta = zeta / zeta[-1]\n",
    "    interv_div = np.linspace(1, n, n_red + 1).astype(np.int64)\n",
    "\n",
    "    # zeta = np.array([zeta[i1-1] for i1 in interv_div[1:]])\n",
    "    zeta = zeta[interv_div[1:] - 1]\n",
    "    return zeta\n",
    "\n",
    "\n",
    "\n",
    "def Zipf_Mand_3S_CDF(n, alpha_1=1.17, alpha_2=1.67, alpha_3=2.33,\n",
    "                           beta=7.1, c1=2153013.1,c2=10052.1,c3=167.25, c4 =98.83,\n",
    "                           N1= 100, N2 = 2000):\n",
    "    \"\"\"Computes Zipf-Mandelbrot cumulative distribution function in three stages\"\"\"\n",
    "    \n",
    "    x = np.arange(1, n + 1) + beta\n",
    "    v1 = c1 * np.power(x[:N1], -alpha_1)\n",
    "    v2 = N1 ** alpha_2 * c2 * np.power(x[N1:N2], -alpha_2) + c4\n",
    "    v3 = N2 ** alpha_3 * c3 * np.power(x[N2:], -alpha_3)\n",
    "\n",
    "    mandelbrot_law_3S = np.concatenate((v1, v2, v3))\n",
    "    zeta = np.cumsum(mandelbrot_law_3S)\n",
    "    return zeta / zeta[-1]\n",
    "\n",
    "# IDEA : to model vocab_size vs age dependency, play both with n and n_red in following function\n",
    "# Use factor for n, n_red ???\n",
    "def Zipf_Mand_3S_CDF_comp(n, alpha_1=1.17, alpha_2=1.67, alpha_3=2.33,\n",
    "                          beta=7.1, c1=2153013.1, c2=10052.1, c3=167.25, c4 =98.83,\n",
    "                          N1= 100, N2 = 2000, n_red=1000):\n",
    "    \"\"\"Computes Zipf-Mandelbrot cumulative distribution function in three stages\n",
    "       It compresses real long interval n into a smaller n_red\n",
    "       by conserving relative percentages by intervals\"\"\"\n",
    "    \n",
    "    x = np.arange(1, n + 1) + beta\n",
    "    v1 = c1 * np.power(x[:N1], -alpha_1)\n",
    "    v2 = N1 ** alpha_2 * c2 * np.power(x[N1:N2], -alpha_2)\n",
    "    v3 = N2 ** alpha_3 * c3 * np.power(x[N2:], -alpha_3)\n",
    "\n",
    "    mandelbrot_law_3S = np.concatenate((v1, v2, v3))\n",
    "    zeta = np.cumsum(mandelbrot_law_3S)\n",
    "    zeta = zeta / zeta[-1]\n",
    "    \n",
    "    interv_div = np.linspace(1, n, n_red + 1).astype(np.int64)\n",
    "    zeta = zeta[interv_div[1:] - 1]\n",
    "    return zeta\n",
    "\n",
    "\n",
    "def randZipf(zipf_cum_distr, numSamples):\n",
    "    \"\"\"fast computation of array of Zipf samples with dim = numSamples\n",
    "    It needs Zipf CDF as input\"\"\"\n",
    "    unif_random_array = np.random.random(numSamples)\n",
    "    return np.searchsorted(zipf_cum_distr, unif_random_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNC corpus research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import deepdish as dd\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import stemmers and progress bar\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "porter = PorterStemmer()\n",
    "porter_sb = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "import pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/PG/Paolo/python_repos/language_proj/BNC/2554/download/Texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduced DB\n",
    "os.chdir(\"/Users/PG/Paolo/python_repos/language_proj/BNC/2553/2553/download/Texts/news\")\n",
    "# news \n",
    "xml_news_elems = []\n",
    "for file in os.listdir():\n",
    "    xml_news_elems.append(ElementTree().parse(file))\n",
    "\n",
    "all_news_words = []\n",
    "for xml_elem in xml_news_elems:\n",
    "    all_news_words.extend([s.text for s in xml_elem.findall('wtext/div/p/s/w')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reduced DB\n",
    "os.chdir(\"/Users/PG/Paolo/python_repos/language_proj/BNC/2553/2553/download/Texts/dem\")\n",
    "# conversation\n",
    "xml_elems = []\n",
    "for file in os.listdir():\n",
    "    xml_elems.append(ElementTree().parse(file))\n",
    "\n",
    "all_words = []\n",
    "for xml_elem in xml_elems:\n",
    "    all_words.extend([s.text for s in xml_elem.findall('stext/div/u/s/w')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [100,1000,10000,50000,100000,250000, 500000, 750000, 999000]\n",
    "#y1 = [np.unique(all_words[:lens]).shape[0]/len(all_words[:lens]) for lens in x]\n",
    "y2 = [np.unique(all_words[:lens]).shape[0] for lens in x]\n",
    "y3 = [np.unique(all_news_words[:lens]).shape[0] for lens in x]\n",
    "\n",
    "plt.plot(x,y2, label=\"conv\")\n",
    "plt.plot(x,y3, label =\"news\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch to full DB directory\n",
    "os.chdir(\"/Users/PG/Paolo/python_repos/language_proj/BNC/2554/download/Texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get list of file names\n",
    "path_filename_list=[]\n",
    "for dirpath, dirnames, filenames in os.walk(\".\"):\n",
    "    for filename in [f for f in filenames if f.endswith(\".xml\")]:\n",
    "        path_filename_list.append(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_file_paths = path_filename_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml_obj = ElementTree().parse(l_file_paths[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:22\n"
     ]
    }
   ],
   "source": [
    "# conversation full CORPUS\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(749)\n",
    "\n",
    "all_full_conv_words = []\n",
    "for file in path_filename_list[3300:4049]:\n",
    "    xml_obj = ElementTree().parse(file)\n",
    "    try:\n",
    "        all_full_conv_words.extend([s.text.replace(\" \",\"\") \n",
    "                                    for s in xml_obj.findall('stext/div/u/s/w')\n",
    "                                    if s.text\n",
    "                                    and not re.findall(r\"[^a-zA-Z'-]+\", s.text.replace(\" \",\"\"))])\n",
    "        pbar.update()\n",
    "    except:\n",
    "        pass\n",
    "#stemmed_s_words = [porter.stem(word) for word in all_full_conv_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1873,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# written texts full corpus\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(700)\n",
    "full_written_words = []\n",
    "for file in path_filename_list[:700]:\n",
    "    xml_obj = ElementTree().parse(file)\n",
    "    full_written_words.extend([w.text.replace(\" \",\"\") \n",
    "                               for w in xml_obj.findall('wtext/div/p/s/w')\n",
    "                               if not re.findall(r\"[^a-zA-Z']+\", w.text)])\n",
    "    pbar.update()\n",
    "\n",
    "stemmed_w_words = [porter.stem(word) for word in full_written_words]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 01:03:54\n"
     ]
    }
   ],
   "source": [
    "# CODE TO GET WORDS FROM CONVERSATIONAL AND WRITTEN ENGLISH FROM FULL DATABASE\n",
    "pbar = pyprind.ProgBar(len(path_filename_list))\n",
    "\n",
    "all_full_s_words = []\n",
    "all_full_w_words = []\n",
    "for file in path_filename_list:\n",
    "    xml_obj = ElementTree().parse(file)\n",
    "    if xml_obj.findall('stext/div/u/s/w'):\n",
    "        all_full_s_words.extend([s.text.replace(\" \",\"\") \n",
    "                                 for s in xml_obj.findall('stext/div/u/s/w') \n",
    "                                 if s.text\n",
    "                                 and not re.findall(r\"[^a-zA-Z']+\", s.text.replace(\" \",\"\"))])\n",
    "        pbar.update()\n",
    "    else:\n",
    "        all_full_w_words.extend([w.text.replace(\" \",\"\") \n",
    "                                 for w in xml_obj.findall('wtext/div/p/s/w')\n",
    "                                 if w.text\n",
    "                                 and not re.findall(r\"[^a-zA-Z']+\", w.text.replace(\" \",\"\"))])\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SAVE RELEVANT INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE AND LOAD FULL DB WORDS\n",
    "# words_full_BNC = {'w':np.array(all_full_w_words), \n",
    "#                   's':np.array(all_full_s_words)}\n",
    "# dd.io.save('words_full_BNC.h5', words_full_BNC)\n",
    "\n",
    "words_full_BNC = dd.io.load('words_full_BNC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEMMING AND UNIQUE COUNTS SPOKEN\n",
    "stemmed_s_words = [porter.stem(word).lower() for word in words_full_BNC['s']]\n",
    "\n",
    "stemmed_sb_s_words = [porter_sb.stem(word).lower() for word in words_full_BNC['s']]\n",
    "\n",
    "uq_s_words = np.unique(stemmed_s_words, return_counts=True)\n",
    "\n",
    "uq_sb_s_words = np.unique(stemmed_sb_s_words, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEMMING AND UNIQUE COUNTS WRITTEN\n",
    "stemmed_w_words = [porter.stem(word).lower() for word in words_full_BNC['w'][:5000000]]\n",
    "\n",
    "stemmed_sb_w_words = [porter_sb.stem(word).lower() for word in words_full_BNC['w'][:5000000]]\n",
    "\n",
    "uq_w_words = np.unique(stemmed_w_words, return_counts=True)\n",
    "\n",
    "uq_sb_w_words = np.unique(stemmed_sb_w_words, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SAVE STEMMED WORDS\n",
    "stemmed_words_full_BNC = {'w':np.array(stemmed_w_words), \n",
    "                          's':np.array(stemmed_s_words)}\n",
    "dd.io.save('stemmed_words_full_BNC.h5', stemmed_words_full_BNC)\n",
    "\n",
    "stemmed_words_SB_full_BNC = {'w':np.array(stemmed_sb_w_words), \n",
    "                             's':np.array(stemmed_sb_s_words)}\n",
    "dd.io.save('stemmed_words_SB_full_BNC.h5', stemmed_words_SB_full_BNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SAVE UNIQUE WORDS\n",
    "unique_words_full_BNC = {'w':uq_w_words, \n",
    "                         's':uq_s_words}\n",
    "dd.io.save('unique_words_full_BNC.h5', unique_words_full_BNC)\n",
    "\n",
    "unique_words_SB_full_BNC = {'w':uq_sb_w_words, \n",
    "                            's':uq_sb_s_words}\n",
    "dd.io.save('unique_words_SB_full_BNC.h5', unique_words_SB_full_BNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LOAD DATA and COMPUTE FIT COEFFICIENTS of PROBAB DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# switch to full DB directory\n",
    "os.chdir(\"/Users/PG/Paolo/python_repos/language_proj/BNC/2554/download/Texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOAD UNIQUE and STEMMED WORDS\n",
    "#uq_words = dd.io.load('unique_words_full_BNC.h5')\n",
    "uq_SB_words = dd.io.load('unique_words_SB_full_BNC.h5')\n",
    "stemmed_SB_words = dd.io.load('stemmed_words_SB_full_BNC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1789,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [100,1000,10000,50000,100000,250000, 500000, 750000, \n",
    "     1000000, 2000000, 3000000, 4000000, 4800000]\n",
    "y_s = [np.unique(stemmed_SB_words['s'][:lens]).shape[0] for lens in x]\n",
    "y_w = [np.unique(stemmed_SB_words['w'][:lens]).shape[0] for lens in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1790,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y_s, label= 'spoken')\n",
    "plt.plot(x, y_w, label = 'written')\n",
    "plt.legend()\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check 100 most relevant words in 's', 'w' vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1793,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uq_SB_words['s'][0][np.argsort(uq_SB_words['s'][1])[::-1]][:100], uq_SB_words['w'][0][np.argsort(uq_SB_words['w'][1])[::-1]][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ZIPF - HEAPS CURVE FITTING -> GET PARAMETERS VALUES\n",
    "def heaps_fun(x, k, beta):\n",
    "    x = np.array(x)\n",
    "    return k * x ** beta\n",
    "\n",
    "x_h = np.array([1,10,100,1000,10000,100000,1000000,4000000])\n",
    "y_h = [np.unique(stemmed_SB_words['s'][:lens]).shape[0] for lens in x_h]\n",
    "\n",
    "(k_h, beta_h),_ = opt.curve_fit(heaps_fun, x_h, y_h, p0=(50, 0.5))\n",
    "\n",
    "xx=np.linspace(1,4000000, 1000)\n",
    "plt.plot(xx, k_h * xx ** beta_h)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS TO FIT POWER LAWS TO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def zipf_fit(x, alpha, c):\n",
    "    #x = x + beta\n",
    "    #return - alpha * np.log(x)\n",
    "    return c * np.power(x, -alpha)\n",
    "\n",
    "def zipf_mand_fit(x, alpha, beta, c):\n",
    "    #x = x + beta\n",
    "    #return - alpha * np.log(x)\n",
    "    x = x + beta\n",
    "    return c * np.power(x, -alpha)\n",
    "\n",
    "def zipf_mand_fit_2S(r, alpha1, alpha2, beta, c1, c2):\n",
    "    N=2000\n",
    "\n",
    "    r = r + beta\n",
    "    v1 = c1 * np.power(r[:N], -alpha1)\n",
    "    v2 = N**alpha2 * c2 * np.power(r[N:], -alpha2)\n",
    "    eps = np.abs(c1 * np.power(r[N-1], -alpha1) - N**alpha2 * c2 * np.power(r[N], -alpha2))\n",
    "    \n",
    "    f_union = np.concatenate((v1, v2))\n",
    "    #f_union[N-1:N+1] += 10*eps\n",
    "        \n",
    "    return f_union + eps\n",
    "\n",
    "def zipf_mand_fit_3S(r, alpha1, alpha2, alpha3, beta, c1, c2, c3, c4):\n",
    "    \n",
    "    N1 = 100\n",
    "    N2 = 2000\n",
    "\n",
    "    r = r + beta\n",
    "    v1 = c1 * np.power(r[:N1], -alpha1)\n",
    "    v2 = N1**alpha2 * c2 * np.power(r[N1:N2], -alpha2) + c4 # need extra DOF to fit conditions\n",
    "    v3 = N2**alpha3 * c3 * np.power(r[N2:], -alpha3)\n",
    "    \n",
    "#     eps1 = np.abs(c1 * np.power(r[N1-1], -alpha1) - y_s[100])\n",
    "#     eps2 = np.abs(N1**alpha2 * c2 * np.power(r[N1], -alpha2) + c4 - y_s[100])\n",
    "#     eps3 = np.abs(N1**alpha2 * c2 * np.power(r[N2-1], -alpha2) + c4 - y_s[2000])\n",
    "#     eps4 = np.abs(N2**alpha3 * c3 * np.power(r[N2], -alpha3) - y_s[2000])\n",
    "    \n",
    "#     eps1 = np.abs(c1 * np.power(r[N1-1], -alpha1) \n",
    "#                   - N1**alpha2 * c2 * np.power(r[N1], -alpha2) - c4)\n",
    "    \n",
    "#     eps2 = np.abs(N1**alpha2 * c2 * np.power(r[N2-1], -alpha2) + c4 \n",
    "#                   - N2**alpha3 * c3 * np.power(r[N2], -alpha3))\n",
    "    \n",
    "    f_union = np.concatenate((v1, v2, v3))\n",
    "    \n",
    "#     f_union[N1-1] += eps1\n",
    "#     f_union[N1] += eps2\n",
    "    \n",
    "#     f_union[N2-1] += eps3\n",
    "#     f_union[N2] += eps4\n",
    "    \n",
    "    #f_union[N1-1: N1 + 1] += 10 * eps1\n",
    "    #f_union[N2-1:N2+1] += 10 * eps2\n",
    "    \n",
    "    return f_union #+ eps1 + eps2 #+ eps3 + eps4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1911,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k2,k4=10000,100\n",
    "plt.plot(x_s,100*1.6*k2 * np.power(x_s, -1.6) + k4)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define DATA to be FIT ( important to do it properly...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_s = 24000\n",
    "n_w = 52000\n",
    "x_s = np.arange(1, n_s + 1)\n",
    "x_w = np.arange(1, n_w + 1)\n",
    "\n",
    "y_s = np.sort(uq_SB_words['s'][1])[::-1][:n_s]\n",
    "y_w = np.sort(uq_SB_words['w'][1])[::-1][:n_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1759,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2979192327416069, 8.7774134919893907, 3511560.4378326912)"
      ]
     },
     "execution_count": 1759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ONE REGIME FIT (SPOKEN)\n",
    "(alpha_s,beta_s, c_s), _ = opt.curve_fit(zipf_mand_fit, x_s, y_s, p0=(1.3, 3, 1000))\n",
    "(alpha_s,beta_s, c_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1760,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2384066155491573,\n",
       " 1.8735935624145901,\n",
       " 7.8422120761183924,\n",
       " 2727267.9722089712,\n",
       " 223.45785615292965)"
      ]
     },
     "execution_count": 1760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TWO REGIMES FIT (SPOKEN)\n",
    "(alpha_s1, alpha_s2, beta_s_imp, c_s1, c_s2), _ = opt.curve_fit(zipf_mand_fit_2S, \n",
    "                                                            x_s, y_s, \n",
    "                                                            p0=(1.3,2.5, 3, 100000, 100))\n",
    "(alpha_s1, alpha_s2, beta_s_imp, c_s1, c_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2222786840696638,\n",
       " 1.4954601266269689,\n",
       " 1.8986458118523217,\n",
       " 7.7972793355069152,\n",
       " 2613014.9799408969,\n",
       " 9668.5668750285349,\n",
       " 122.1289147117548,\n",
       " 11.429085154568304)"
      ]
     },
     "execution_count": 2258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THREE REGIMES FIT (SPOKEN)\n",
    "sigma =np.ones(len(y_s))\n",
    "sigma[[99, 100, 101, 500, 1700, 1999, 2000, 2001, 2002,  -1]] = 0.001\n",
    "(a3S_1, a3S_2,a3S_3,beta_3S, c3S_1, c3S_2, c3S_3, C4_S), _ = opt.curve_fit(zipf_mand_fit_3S, \n",
    "                                                                           x_s, y_s, \n",
    "                                                                           p0=(1., 1.3, 2.5, \n",
    "                                                                               3, \n",
    "                                                                               100000, 1000, 100, 10),\n",
    "                                                                           sigma=sigma)\n",
    "(a3S_1, a3S_2,a3S_3,beta_3S, c3S_1, c3S_2, c3S_3, C4_S)\n",
    "\n",
    "\n",
    "# (a3S_1, a3S_2,a3S_3,beta_3S, c3S_1, c3S_2, c3S_3), _ = opt.curve_fit(zipf_mand_fit_3S, \n",
    "#                                                                            x_s, y_s, \n",
    "#                                                                            p0=(1., 1.3, 2.5, \n",
    "#                                                                                3, \n",
    "#                                                                                100000, 1000, 100),\n",
    "#                                                                            sigma=sigma)\n",
    "# (a3S_1, a3S_2,a3S_3,beta_3S, c3S_1, c3S_2, c3S_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c7e049e8>"
      ]
     },
     "execution_count": 2318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA vs FIT COMPARISON (SPOKEN) \n",
    "plt.plot(x_s, y_s, label = 'data')\n",
    "plt.plot(zipf_mand_fit_3S(x_s, a3S_1, a3S_2, a3S_3, beta_3S, c3S_1, c3S_2, c3S_3, C4_S), \n",
    "         label = '3S_fit')\n",
    "\n",
    "# plt.plot(zipf_mand_fit_2S(x_s, alpha_s1, alpha_s2, beta_s_imp, c_s1, c_s2), label='2S_fit')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('word rank')\n",
    "plt.ylabel('word frequency')\n",
    "plt.title('Data vs fit in spoken words corpus')\n",
    "plt.grid('on')\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "CONCLUSION : Z-M fitting overestimates frequency from r = 1000 onwards\n",
    "There are two regimes in linguistic power law. \n",
    "Shift occurs around r = 2000-3000\n",
    "See papers by MONTEMURRO : 'Beyond the Zipf-Mandelbrot law in quantitative linguistics'\n",
    "and FERRER-I-CANCHO and SOLE : 'Two regimes in frequency of words'\n",
    "Real fitting is cumbersome -> Need to simplify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize power law dependency on vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1802,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x289754ba8>"
      ]
     },
     "execution_count": 1802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for n_red in [50,1000,5000,15000,24000]:\n",
    "    interv_div = np.linspace(1, 24000, n_red + 1).astype(np.int64)\n",
    "    new_ys = y_s[interv_div[1:] - 1]\n",
    "    plt.plot(range(n_red),new_ys, label = str(n_red) + 'words')\n",
    "    \n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1719,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9091269738214558,\n",
       " 1.4817794053535509,\n",
       " 0.33552797658385036,\n",
       " 413393.53629757592,\n",
       " 412.73527425955137)"
      ]
     },
     "execution_count": 1719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2S WRITTEN FIT\n",
    "(alpha_w1, alpha_w2, beta_w_imp, c_w1, c_w2), _ = opt.curve_fit(zipf_mand_fit_2S, \n",
    "                                                            x_w, y_w, \n",
    "                                                            p0=(1.,2., 3, 100000, 100))\n",
    "(alpha_w1, alpha_w2, beta_w_imp, c_w1, c_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98779924555558052,\n",
       " 0.78013700474286507,\n",
       " 1.6496670400187725,\n",
       " 0.60344865923415125,\n",
       " 501780.35693374212,\n",
       " 5544.6062437143801,\n",
       " 279.60203407490104,\n",
       " -259.15257421842836)"
      ]
     },
     "execution_count": 2139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3S WRITTEN FIT\n",
    "sigma =np.ones(len(y_w))\n",
    "sigma[[99, 100, 101, 500, 1700, 1999, 2000, 2001, 2002, -1]] = 0.001\n",
    "(a3S_w1, a3S_w2,a3S_w3,beta_3w, c3S_w1, c3S_w2, c3S_w3, C4_w), _ = opt.curve_fit(zipf_mand_fit_3S, \n",
    "                                                            x_w, y_w, \n",
    "                                                            p0=(1.,1.3,2.5, 3, 100000, 1000,100, 10),\n",
    "                                                            sigma=sigma)\n",
    "(a3S_w1, a3S_w2,a3S_w3,beta_3w, c3S_w1, c3S_w2, c3S_w3, C4_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b8f41b70>"
      ]
     },
     "execution_count": 2140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA vs FIT COMPARISON WRITTEN\n",
    "plt.plot(x_w,y_w, label = 'data')\n",
    "\n",
    "plt.plot(zipf_mand_fit_3S(x_w, a3S_w1, a3S_w2,a3S_w3,beta_3w, c3S_w1, c3S_w2, c3S_w3, C4_w), \n",
    "         label = '3_region_fit')\n",
    "plt.title('Data vs fit in written words corpus')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMPARE SPOKEN vs WRITTEN POWER LAWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2cfab9048>"
      ]
     },
     "execution_count": 2321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(zipf_mand_fit_3S(x_s, a3S_1, a3S_2, a3S_3, beta_3S, c3S_1, c3S_2, c3S_3, C4_S), \n",
    "         label = '3R_fit_S')\n",
    "plt.plot(zipf_mand_fit_3S(x_w, a3S_w1, a3S_w2,a3S_w3,beta_3w, c3S_w1, c3S_w2, c3S_w3, C4_w), \n",
    "         label = '3R_fit_W')\n",
    "\n",
    "# plt.plot(zipf_mand_fit_2S(x_s, alpha_s1, alpha_s2, beta_s_imp, c_s1, c_s2), label='2S_fit')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('word rank')\n",
    "plt.ylabel('word frequency')\n",
    "plt.title('Data vs fit in spoken words corpus')\n",
    "plt.grid('on')\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring correct order of COMPRESSED merged s, w VOCAB array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SET DIFFERENCE WRITTEN-SPOKEN\n",
    "set_diff=np.setdiff1d(uq_SB_words['w'][0], uq_SB_words['s'][0], assume_unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INTERSECTION WRITTEN-SPOKEN\n",
    "set_inters=np.intersect1d(uq_SB_words['w'][0], uq_SB_words['s'][0], assume_unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_s = np.array(['aa','xx','cd','cc','ww','yy'])\n",
    "w_s_counts = np.array([33,22,7,3,2,1], dtype=np.uint32)\n",
    "\n",
    "w_s_dict = dict(zip(w_s, w_s_counts))\n",
    "\n",
    "w_w = np.array(['cc','bb','aa','cd','tw','ww'])\n",
    "w_w_counts = np.array([25,18,14,9,5,1], dtype=np.uint32)\n",
    "\n",
    "w_w_dict = dict(zip(w_w, w_w_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'bb' 'cc' 'cd' 'tw' 'ww' 'xx' 'yy'] [47 18 28 16  5  3 22  1]\n"
     ]
    }
   ],
   "source": [
    "# union\n",
    "union_vocab = np.union1d(w_s, w_w)\n",
    "union_vocab_c = np.zeros(union_vocab.shape[0], dtype=np.uint32)\n",
    "\n",
    "\n",
    "\n",
    "# get total counts of intersect\n",
    "\n",
    "# union_vocab_c[np.in1d(union_vocab, w_s)] += w_s_counts\n",
    "# union_vocab_c[np.in1d(union_vocab, w_w)] += w_w_counts\n",
    "\n",
    "for idx, word in enumerate(union_vocab):\n",
    "    try:\n",
    "        c = w_s_dict[word]\n",
    "        union_vocab_c[idx] += c\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for idx, word in enumerate(union_vocab):\n",
    "    try:\n",
    "        c = w_w_dict[word]\n",
    "        union_vocab_c[idx] += c\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(union_vocab, union_vocab_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['aa', 'cc', 'xx', 'bb', 'cd', 'tw', 'ww', 'yy'], \n",
       "       dtype='<U2'), array([47, 28, 22, 18, 16,  5,  3,  1], dtype=uint32))"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_vocab=union_vocab[np.argsort(union_vocab_c)[::-1]]\n",
    "union_vocab_c = union_vocab_c[np.argsort(union_vocab_c)[::-1]]\n",
    "union_vocab, union_vocab_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True,  True,  True,  True,  True,  True, False, False], dtype=bool),\n",
       " array([ True,  True,  True,  True,  True,  True], dtype=bool))"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total counts of intersect\n",
    "np.in1d(union_vocab, w_w), np.in1d(w_w, union_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_in_w = np.in1d(w_s,w_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_in_s = np.in1d(w_w,w_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'bb', 'cd', 'ww'], \n",
       "      dtype='<U2')"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_s[s_in_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_w_counts[w_in_s] += w_s_counts[s_in_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 40, 14, 16,  5,  3])"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_w_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 40, 16,  3])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_common_counts = w_w_counts[w_in_s] + w_s_counts[s_in_w]\n",
    "tot_common_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add common elements counts from 's' to 'w'  \n",
    "w_w_counts[w_in_s] += w_s_counts[s_in_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_s_counts[~np.in1d(w_s,w_w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14,  5])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_w_counts[~np.in1d(w_w,w_s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 40, 14, 16,  5,  3])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_w_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'bb', 'cd', 'ww'], \n",
       "      dtype='<U2')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_w[w_in_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33, 22,  7,  3,  2])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_s_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = np.array([2,4,6,8])\n",
    "v2 = np.array([10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1[[1,3]] += v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_in_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.searchsorted(tot_common_counts[::-1], w_s_counts[~np.in1d(w_s,w_w)], side = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 16, 40, 58])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_common_counts[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_s_counts[~np.in1d(w_s,w_w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 40, 16,  3,  3,  1])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((tot_common_counts, w_s_counts[~np.in1d(w_s,w_w)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 40, 16,  3, 14,  5])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((tot_common_counts, w_w_counts[~np.in1d(w_w,w_s)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 4, 2, 1, 0])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx=np.concatenate((tot_common_counts, w_w_counts[~np.in1d(w_w,w_s)]))\n",
    "np.argsort(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'\", \"'a\", \"'d\", ..., 'solhal', 'solicit', 'solicitor'], \n",
       "      dtype='<U104')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(uq_SB_words['s'][0])[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'\", \"''\", \"'a\", ..., 'rowley', 'rowlock', 'rowntre'], \n",
       "      dtype='<U25')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(uq_SB_words['w'][0])[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sort_s = uq_SB_words['s'][0][np.argsort(uq_SB_words['s'][1])[::-1]][:20000]\n",
    "sort_s_c=uq_SB_words['s'][1][np.argsort(uq_SB_words['s'][1])[::-1]][:20000]\n",
    "\n",
    "sort_w = uq_SB_words['w'][0][np.argsort(uq_SB_words['w'][1])[::-1]][:40000]\n",
    "sort_w_c=uq_SB_words['w'][1][np.argsort(uq_SB_words['w'][1])[::-1]][:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15692,)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(sort_s,sort_w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.in1d(sort_s,sort_w).shape # s-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.in1d(sort_w,sort_s).shape # w-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.sort(uq_SB_words['s'][1])[::-1][:20000][np.in1d(sort_s,sort_w)[:20000]])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd=np.nonzero(np.in1d(sort_s,sort_w))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x194854278>]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['mhm', 'int', 'urgh', 'tt', 'aargh', 'bryoni', 'bom', 'marg', 'umm',\n",
       "        'yum', 'kath', 'hiya', 'nope', 'ding', \"an'al\", 'gemma', 'lego',\n",
       "        'poo', 'sshh', 'oy', 'ahhh', 'jehovah', 'pauli', 'cath', 'shel',\n",
       "        'uhum', 'gwendolin', 'hayley', 'heidi', 'snog', 'foxi', 'shrimpi',\n",
       "        'awl', 'fahrenheit', 'hallo', 'ey', 'deana', 'dinda', 'lari',\n",
       "        'pott', 'ehm', 'golli', \"t'\", 'choo', 'ner', 'joell', 'winsi',\n",
       "        'corrinn', 'plonker', 'whatsernam', 'whoo', 'southwold', 'tarrah',\n",
       "        'frig', 'woh', 'markham', 'bedg', 'krispi', 'lyndsey', 'ashington',\n",
       "        'magilton', 'kaley', 'almondsburi', 'chuff', 'scanner', 'melissa',\n",
       "        'dong', 'gorbachov', 'netto', 'quoit', 'urc', 'stowmarket',\n",
       "        'westbound', 'poxi', 'timmi', 'popsey', 'thunderbird', 'macro',\n",
       "        'yuck', 'buby', 'lawford', 'ohhh', 'twat', 'nanna', 'shandi',\n",
       "        'whee', 'arf', 'skint', 'mikila', 'dempsey', 'hatti', 'kimmi',\n",
       "        'racetrack', 'carterton', 'beezer', 'miocenia', 'codi', 'poppet',\n",
       "        'nuffink', 'recherch'], \n",
       "       dtype='<U104'),\n",
       " array([1919,  568,  438,  306,  157,  146,  135,  118,  111,  103,   89,\n",
       "          86,   86,   80,   73,   70,   67,   64,   63,   53,   52,   52,\n",
       "          50,   49,   47,   45,   42,   42,   41,   39,   39,   39,   38,\n",
       "          38,   38,   37,   37,   33,   33,   32,   32,   31,   31,   31,\n",
       "          30,   29,   29,   28,   28,   28,   28,   27,   27,   26,   26,\n",
       "          26,   25,   25,   25,   25,   25,   25,   24,   24,   24,   24,\n",
       "          23,   23,   23,   22,   22,   22,   22,   22,   22,   22,   22,\n",
       "          22,   21,   21,   21,   21,   21,   21,   21,   21,   21,   21,\n",
       "          20,   20,   20,   20,   20,   20,   20,   20,   20,   19,   19,\n",
       "          19]))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_s[~np.in1d(sort_s,sort_w)][:100], sort_s_c[~np.in1d(sort_s,sort_w)][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['mungo', 'ec', 'nordern', 'erika', 'gazzer', 'engel', 'tammuz',\n",
       "        'mcleish', 'wexford', 'moran', 'br', 'herr', 'noriega', 'panama',\n",
       "        'frau', 'karajan', 'nhs', 'ii', 'alida', 'pragu', 'roirbak',\n",
       "        'leila', 'hatton', 'ferranti', 'dorothea', 'elisabeth',\n",
       "        'nonconformist', 'menzi', 'hir', 'clarissa', 'gaili', 'tremayn',\n",
       "        'hobb', 'caspar', 'warsaw', 'ira', 'omi', 'holliday', 'itv', 'bodo',\n",
       "        'marxist', 'jos', 'depict', 'libel', 'lb', 'harker', 'guberniia',\n",
       "        'cullam', 'yeo', 'ceausescu', 'masha', 'qc', 'gm', 'fanshaw',\n",
       "        'clow', 'natwest', 'eurotunnel', 'tolkien', 'dti', 'panamanian',\n",
       "        'surkov', 'rozanov', 'jahsaxa', 'knighton', 'koon', 'crevecoeur',\n",
       "        'havel', 'anc', 'mitterrand', 'civilis', 'pr', 'faldo', 'cooney',\n",
       "        'malamut', 'kinship', 'plo', 'strasbourg', 'clinton', 'hindu',\n",
       "        'defri', 'nep', 'derrida', 'politburo', 'francesca', 'parvi',\n",
       "        'aldington', 'gassendi', 'exclaim', 'frankfurt', 'underwrit',\n",
       "        'smolensk', 'honeck', 'highland', 'budapest', 'ibm', 'elinor',\n",
       "        'entrepreneur', 'tiananmen', 'augusta', 'hammersmith'], \n",
       "       dtype='<U25'),\n",
       " array([546, 443, 432, 421, 338, 330, 310, 308, 307, 296, 289, 247, 237,\n",
       "        235, 226, 217, 205, 197, 190, 184, 184, 180, 179, 177, 176, 172,\n",
       "        172, 168, 167, 159, 158, 152, 151, 147, 145, 144, 143, 140, 140,\n",
       "        131, 130, 127, 124, 123, 123, 123, 116, 113, 113, 112, 112, 112,\n",
       "        110, 109, 108, 107, 106, 104, 101, 101, 101, 101, 100,  99,  98,\n",
       "         98,  98,  97,  96,  94,  94,  94,  92,  91,  90,  89,  89,  89,\n",
       "         89,  88,  88,  88,  88,  87,  86,  86,  85,  84,  84,  83,  83,\n",
       "         82,  82,  81,  81,  80,  79,  78,  78,  77]))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_w[~np.in1d(sort_w,sort_s)][:100], sort_w_c[~np.in1d(sort_w,sort_s)][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sort_s_c=uq_SB_words['s'][1][np.argsort(uq_SB_words['s'][1])[::-1]][:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1919,  568,  438,  306,  157,  146,  135,  118,  111,  103,   89,\n",
       "         86,   86,   80,   73,   70,   67,   64,   63,   53,   52,   52,\n",
       "         50,   49,   47,   45,   42,   42,   41,   39,   39,   39,   38,\n",
       "         38,   38,   37,   37,   33,   33,   32,   32,   31,   31,   31,\n",
       "         30,   29,   29,   28,   28,   28,   28,   27,   27,   26,   26,\n",
       "         26,   25,   25,   25,   25,   25,   25,   24,   24,   24,   24,\n",
       "         23,   23,   23,   22,   22,   22,   22,   22,   22,   22,   22,\n",
       "         22,   21,   21,   21,   21,   21,   21,   21,   21,   21,   21,\n",
       "         20,   20,   20,   20,   20,   20,   20,   20,   20,   19,   19,\n",
       "         19])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_s_c[~np.in1d(sort_s,sort_w)][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([1287]),), (array([2807]),))"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(sort_w == 'maintain'), np.where(sort_s == 'maintain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(477, 39301)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_w_c[1287], sort_s_c[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word_string = ['maintain', 'keep', 'sort', 'put', 'inherit', 'get', 'obtain', 'up', 'down', 'shit',\n",
    "#               'go', 'enhanc', 'enabl', 'nice', 'cute', 'good', 'can', 'stink', 'odour',\n",
    "#               'sick', 'guy', 'bloke', 'see', 'rare', 'peril', 'vicious', 'wash', 'gum',\n",
    "#               'tranquil', 'bold', 'brave', 'sort', 'set', 'feel','want', 'like', 'thus',\n",
    "#               'wonder', 'do', 'make', 'cheap', 'busy', 'again', 'urg', 'rush', 'top',\n",
    "#               'bottom', 'should', 'it', 'think', 'look', 'meat', 'insert', 'wide', 'ampl',\n",
    "#               'mouth', 'fat', 'pleas', 'fart', 'blow', 'bastard', 'poor', 'sit', 'stay',\n",
    "#               'super', 'atroc', 'pain', \"catch\", 'miss', 'mean', 'seek', 'sought', 'search',\n",
    "#               'better', 'wors', 'neat', 'clean', 'skew', 'random', 'zero', 'bad', 'finish',\n",
    "#               'end', 'commenc', 'trip', 'on', 'special', 'car', 'word', 'henc', 'well', 'by',\n",
    "#               'peopl', 'give', 'back', 'some', 'most', 'just']\n",
    "\n",
    "# print('word', 'spoken', 'written')\n",
    "# for word in word_string:\n",
    "#     print(word, sort_s_c[np.argwhere(sort_s == word)], sort_w_c[np.argwhere(sort_w == word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int_words=np.intersect1d(sort_s,sort_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_w=[]\n",
    "for word in int_words[:100]:\n",
    "    i_w, i_s = np.argwhere(sort_w == word), np.argwhere(sort_s == word)\n",
    "    count_w = sort_w_c[i_w][0][0]\n",
    "    count_s = sort_s_c[i_s][0][0]\n",
    "    if count_s and count_w:\n",
    "        if (count_w >= 20 or count_s >= 20) & (count_w / count_s >=5):\n",
    "            l_w.append((sort_w[i_w][0][0],\n",
    "                        sort_s_c[i_s][0][0], \n",
    "                        sort_w_c[i_w][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#l_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_s=[]\n",
    "for word in int_words:\n",
    "    i_w, i_s = np.argwhere(sort_w == word), np.argwhere(sort_s == word)\n",
    "    count_w = sort_w_c[i_w][0][0]\n",
    "    count_s = sort_s_c[i_s][0][0]\n",
    "    if count_s and count_w:\n",
    "        if (count_w >= 100 or count_s >= 100) & ( count_s / count_w >= 5):\n",
    "            l_s.append((sort_s[i_s][0][0],\n",
    "                        sort_s_c[i_s][0][0], \n",
    "                        sort_w_c[i_w][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#l_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = np.array(['aa','rr','ww'])\n",
    "a2 = np.array(['qq','aa','ii','ww','ll','hh','rr'])\n",
    "a3= ['qq','aa','ii','ww','ll','hh','rr']\n",
    "# wanted result [1, 6, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 loops, best of 3: 8.57 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit [a3.index(x) for x in a1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6, 3]"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.where(a2==x)[0][0] for x in a1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(a2 == a1[0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 6])"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.in1d(a2, a1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ANOTHER METHOD TO COMBINE S-W VOCAB\n",
    "\n",
    "w_s = np.array(['aa','xx','cd','cc','ww','yy'])\n",
    "w_s_counts = np.array([33,22,7,3,2,1], dtype=np.uint32)\n",
    "\n",
    "w_w = np.array(['cc','bb','aa','cd','tw','ww'])\n",
    "w_w_counts = np.array([25,18,14,9,5,1], dtype=np.uint32)\n",
    "# find union\n",
    "union = np.union1d(w_s, w_w)\n",
    "union_c = np.zeros(union.shape[0], dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_s = [np.where(union==x)[0][0] for x in w_s]\n",
    "t_w = [np.where(union==x)[0][0] for x in w_w]\n",
    "\n",
    "union_c[t_s] += w_s_counts\n",
    "union_c[t_w] += w_w_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ix_sort_union = np.argsort(union_c)[::-1]\n",
    "union_sorted = union[ix_sort_union]\n",
    "union_c_sorted = union_c[ix_sort_union]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_s2 = [np.where(union_sorted==x)[0][0] for x in w_s]\n",
    "t_w2 = [np.where(union_sorted==x)[0][0] for x in w_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get samples fom each dist\n",
    "r_s = np.random.randint(0,5, 6).astype(np.uint32)\n",
    "r_w = np.random.randint(0,10, 6).astype(np.uint32)\n",
    "\n",
    "union_c_sorted[t_s2] += r_s\n",
    "union_c_sorted[t_w2] += r_w\n",
    "\n",
    "union_sorted, union_c_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and COMPRESS s and w vocabs: Application to REAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort unique words and corresponding counts (limit s and w to 20k and 40k respectively)\n",
    "sort_s = uq_SB_words['s'][0][np.argsort(uq_SB_words['s'][1])[::-1]][:20000]\n",
    "sort_s_c=uq_SB_words['s'][1][np.argsort(uq_SB_words['s'][1])[::-1]][:20000].astype(np.uint32)\n",
    "\n",
    "sort_w = uq_SB_words['w'][0][np.argsort(uq_SB_words['w'][1])[::-1]][:40000]\n",
    "sort_w_c=uq_SB_words['w'][1][np.argsort(uq_SB_words['w'][1])[::-1]][:40000].astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(sort_s_c)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.loglog(np.arange(len(sort_s_c)),sort_s_c)\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15692,)"
      ]
     },
     "execution_count": 2174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute intersection set length\n",
    "np.intersect1d(sort_s, sort_w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2d2167518>"
      ]
     },
     "execution_count": 2187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(np.nonzero(np.in1d(sort_s, sort_w))[0], np.ones(15692))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1993.,  1970.,  1900.,  1808.,  1689.,  1546.,  1435.,  1258.,\n",
       "         1140.,   953.]),\n",
       " array([     0. ,   1999.9,   3999.8,   5999.7,   7999.6,   9999.5,\n",
       "         11999.4,  13999.3,  15999.2,  17999.1,  19999. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 2190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs=np.histogram(np.nonzero(np.in1d(sort_s, sort_w))[0], bins=40)\n",
    "plt.hist(np.nonzero(np.in1d(sort_s, sort_w))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute union of different word types and initialize total global counts\n",
    "union_real = np.union1d(sort_s, sort_w)\n",
    "union_real_c = np.zeros(union_real.shape[0], dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute transformations from lang_type sorted_lists to union_list\n",
    "t_s = [np.where(union_real==x)[0][0] for x in sort_s]\n",
    "t_w = [np.where(union_real==x)[0][0] for x in sort_w]\n",
    "\n",
    "# update counts\n",
    "union_real_c[t_s] += sort_s_c\n",
    "union_real_c[t_w] += sort_w_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sort union arrays\n",
    "ix_sort_union = np.argsort(union_real_c)[::-1]\n",
    "union_real_sorted = union_real[ix_sort_union]\n",
    "union_real_c_sorted = union_real_c[ix_sort_union]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot union vocabulary\n",
    "plt.plot(union_real_c_sorted)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute transformations from lang_type sorted_lists to SORTED union_list\n",
    "t_s_un_sort = [np.where(union_real_sorted==x)[0][0] for x in sort_s]\n",
    "t_w_un_sort = [np.where(union_real_sorted==x)[0][0] for x in sort_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_s_un_sort = np.array(t_s_un_sort)\n",
    "t_w_un_sort = np.array(t_w_un_sort)\n",
    "transf_to_sorted_union = {'s':t_s_un_sort, 'w':t_w_un_sort}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd.io.save('transf_to_sorted_vocab_union.h5', transf_to_sorted_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transf_to_sorted_union = dd.io.load('transf_to_sorted_vocab_union.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# limit total words to 40000\n",
    "t_s_un_sort_LIM = np.array([x for x in t_s_un_sort if x<40000])\n",
    "t_w_un_sort_LIM = np.array([x for x in t_w_un_sort if x<40000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2bb6a20f0>]"
      ]
     },
     "execution_count": 2149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.union1d(t_s_un_sort_LIM, t_w_un_sort_LIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0, ..., 981, 567, 730], dtype=uint32),\n",
       " array([  0,   0,   0, ..., 993, 719, 993], dtype=uint32))"
      ]
     },
     "execution_count": 1038,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t_s_un_sort_LIM/40).astype(np.uint32), (t_w_un_sort_LIM/40).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ba4ef2e8>]"
      ]
     },
     "execution_count": 2150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot((t_s_un_sort_LIM/40).astype(np.uint32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_s=pd.DataFrame({'s':(t_s_un_sort_LIM/40).astype(np.uint32)})\n",
    "df_w=pd.DataFrame({'w':(t_w_un_sort_LIM/40).astype(np.uint32)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2bc011080>"
      ]
     },
     "execution_count": 2152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s.groupby('s').size().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2bc011080>"
      ]
     },
     "execution_count": 2153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w.groupby('w').size().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 8, 2, 0, 0, 0, 2, 0, 0, 0])"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randZipf(cdfs['s'][1000],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ix_s=(np.array(t_s_un_sort_LIM)/40).astype(np.uint32) \n",
    "ix_w=(np.array(t_w_un_sort_LIM)/40).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36487,)"
      ]
     },
     "execution_count": 1255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/PG/Paolo/python_repos/language_proj/lang_model_simple\")\n",
    "cdfs = dd.io.load('cdfs_3R_vs_step.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i_s, c_s = np.unique(randZipf(cdfs['s'][1500], 1000), return_counts=True)\n",
    "i_w, c_w = np.unique(randZipf(cdfs['w'][1500], 1000), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([30, 10]), array([ 6, 17, 13,  4]), array([ 1,  4, 16, 14,  3,  2]))"
      ]
     },
     "execution_count": 1112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(ix_w[0:40]),np.bincount(ix_w[40:80]),np.bincount(ix_w[80:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1042.2857142857142, 506.3157894736842)"
      ]
     },
     "execution_count": 1301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36480/35, 19240/38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#l_s_max_bins=np.array([(np.argmax(np.bincount(ix_s[i:i+40]))) for i in range(0,20000,40)])\n",
    "#l_w_max_bins=np.array([(np.argmax(np.bincount(ix_w[i:i+40]))) for i in range(0,40000,40)])\n",
    "l_s_max_bins = []\n",
    "for i in range(0,19240,38):\n",
    "    bin_c = np.bincount(ix_s[i:i+38])\n",
    "    len_bins = np.nonzero(bin_c)[0].shape[0]\n",
    "    idxs_maxs = heapq.nlargest(len_bins, range(len(bin_c)), bin_c.take)\n",
    "    #print(idxs_maxs)\n",
    "    for elem in idxs_maxs:\n",
    "        if elem in l_s_max_bins:\n",
    "            pass\n",
    "        else:\n",
    "            l_s_max_bins.append(elem)\n",
    "            break\n",
    "l_s_max_bins = np.array(l_s_max_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_w_max_bins = []\n",
    "for i in range(0,36480,35):\n",
    "    bin_c = np.bincount(ix_w[i:i+35])\n",
    "    len_bins = np.nonzero(bin_c)[0].shape[0]\n",
    "    idxs_maxs = heapq.nlargest(len_bins, range(len(bin_c)), bin_c.take)\n",
    "    for elem in idxs_maxs:\n",
    "        if elem in l_w_max_bins:\n",
    "            pass\n",
    "        else:\n",
    "            l_w_max_bins.append(elem)\n",
    "            break\n",
    "l_w_max_bins = np.array(l_w_max_bins)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 506, 990, 990)"
      ]
     },
     "execution_count": 1355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_s_max_bins), len(np.unique(l_s_max_bins)), len(l_w_max_bins), len(np.unique(l_w_max_bins))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19c214fd0>]"
      ]
     },
     "execution_count": 1356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(l_s_max_bins)\n",
    "plt.plot(l_w_max_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((997,), (499,))"
      ]
     },
     "execution_count": 1431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.union1d(l_s_max_bins, l_w_max_bins)).shape, np.unique(np.intersect1d(l_s_max_bins, l_w_max_bins)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a3370630>]"
      ]
     },
     "execution_count": 1476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(l_s_max_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x180434b00>]"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_w_max_bins.sort()\n",
    "plt.plot(l_w_max_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 2, 2, 1, 2, 0, 1,\n",
       "       2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 3, 1, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 1,\n",
       "       2, 2, 2, 2, 2, 2, 3, 1], dtype=uint32)"
      ]
     },
     "execution_count": 1059,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_s[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 982,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(ix_s[19240:19280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381556"
      ]
     },
     "execution_count": 1117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.union1d(l_s_max_bins, l_w_max_bins).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506,), (990,))"
      ]
     },
     "execution_count": 1357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(l_s_max_bins).shape, np.unique(l_w_max_bins).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 9, 6, 9])"
      ]
     },
     "execution_count": 1125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([9, 4, 4, 3, 3, 9, 0, 4, 6, 0])\n",
    "ind = np.argpartition(a, -4)[-4:]\n",
    "a[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1875,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28c2a9630>]"
      ]
     },
     "execution_count": 1875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(cdfs['s'][1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get random samples from s, w\n",
    "num_samp=5000000\n",
    "r_z_s=randZipf(cdfs['s'][1000], num_samp)\n",
    "r_z_w=randZipf(cdfs['w'][1000], num_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_z_s_uq=np.unique(r_z_s, return_counts=True)\n",
    "r_z_w_uq=np.unique(r_z_w, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(r_z_w_uq[1])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial method\n",
    "merged_vocab=np.zeros(1000,dtype=np.int64)\n",
    "merged_vocab[:r_z_s_uq[1].shape[0]] += r_z_s_uq[1]\n",
    "merged_vocab[:r_z_w_uq[1].shape[0]] += r_z_w_uq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((498,), 498)"
      ]
     },
     "execution_count": 1509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_z_s_uq[1].shape, r_z_s_uq[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mixed method \n",
    "merged_vocab=np.zeros(1000,dtype=np.int64)\n",
    "merged_vocab[l_s_max_bins[r_z_s_uq[0]]] += r_z_s_uq[1]\n",
    "merged_vocab[l_w_max_bins[r_z_w_uq[0]]] += r_z_w_uq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(merged_vocab)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#l_s_max_bins[r_z_s_uq[0]], l_w_max_bins[r_z_w_uq[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heapq.nlargest(len(r_z), range(len()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.unique(r_z, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = np.zeros(500).astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.bincount(r_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bc=np.bincount(r_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 10.00 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "100000 loops, best of 3: 3.35 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit z[:bc.shape[0]] += bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### transform from s to w ( assuming w includes all s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_s = randZipf(cdfs['s'][1000],1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.bincount(rand_s) # much faster than np.unique ( factor ~ 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 loops, best of 3: 5.02 s per loop\n"
     ]
    }
   ],
   "source": [
    "np.nonzero(np.bincount(rand_s)) # np.nonzero + np.bincount ~~ 1/8 than np.unique(counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  31,  32,  34,  35,  39,  40,  42,  44,  45,  46,\n",
       "         47,  49,  52,  55,  56,  57,  58,  59,  62,  66,  67,  68,  72,\n",
       "         75,  78,  80,  81,  83,  86,  89,  92,  93,  95,  97, 103, 114,\n",
       "        118, 130, 132, 135, 137, 139, 141, 158, 167, 181, 201, 209, 214,\n",
       "        230, 243, 254, 276, 300, 345, 356, 434]),\n",
       " array([494, 137,  64,  42,  35,  19,  18,  11,  12,  14,   8,  11,   7,\n",
       "          4,   3,   4,   6,   5,   2,   7,   3,   2,   3,   3,   3,   3,\n",
       "          2,   1,   2,   5,   2,   3,   2,   2,   2,   3,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   2,   1,   1,   1,   2,   1,   1,   1,   1,\n",
       "          2,   2,   1,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1]))"
      ]
     },
     "execution_count": 2245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(rand_s, return_counts=True) # slower than bincount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     1,     2, ..., 19994, 19997, 19999]),)"
      ]
     },
     "execution_count": 2234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(np.in1d(sort_s, sort_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_s_w = [np.where(sort_w==x)[0][0] for x in sort_s if x in sort_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d26794e0>]"
      ]
     },
     "execution_count": 2198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(pd.rolling_mean(np.array(t_s_w), 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  13,    0,   27,    6,   10,    3,    8,    2,    4,   45,   49,\n",
       "       4143,    5,    1,   12,   26,    9,  667,   23,   55,   38,   63,\n",
       "          7,   14,   37,  141,  116,   28,  167,   22,   71,   42,   11,\n",
       "         16,   57,  271,   95,   50,   30,   24])"
      ]
     },
     "execution_count": 2208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(t_s_w)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['i', 'the', 'you', 'it', \"'s\", 'and', 'that', 'to', 'a', \"n't\",\n",
       "        'do', 'yeah', 'in', 'of', 'he', 'they', 'is', 'oh', 'have', 'what',\n",
       "        'we', 'no', 'was', 'on', 'there', 'well', 'know', 'she', 'got',\n",
       "        'but', 'go', 'one', 'for', 'be', 'so', 've', 'get', 'like', 'this',\n",
       "        'not'], \n",
       "       dtype='<U104'),\n",
       " array(['the', 'of', 'to', 'and', 'a', 'in', 'it', 'was', 'that', 'is',\n",
       "        \"'s\", 'for', 'he', 'i', 'on', 'with', 'be', 'his', 'as', 'had',\n",
       "        'at', 'by', 'but', 'have', 'not', 'from', 'they', 'you', 'she',\n",
       "        'are', 'this', 'an', 'which', 'her', 'were', 'said', 'has', 'there',\n",
       "        'we', 'would'], \n",
       "       dtype='<U25'))"
      ]
     },
     "execution_count": 2211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_s[:40], sort_w[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"t_s_w\": np.array(t_s_w)})\n",
    "\n",
    "# Bin the data frame by \"a\" with 10 bins...\n",
    "bins = np.linspace(df.t_s_w.min(), df.t_s_w.max(), 392)\n",
    "groups = df.groupby(np.digitize(df.t_s_w, bins))\n",
    "\n",
    "# Get the mean of each bin:\n",
    "print (groups.mean()/40) # Also could do \"groups.aggregate(np.mean)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(groups.mean()/40).round().t_s_w.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5510204081632653, 2.25, 392.0)"
      ]
     },
     "execution_count": 2227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000/392, 54000/24000, 15680/40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling ALGO for WORD MEMORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vocab size changes with age, but power law expression scales to adapt to increasing vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/PG/Paolo/python_repos/language_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2222786840696638,\n",
       " 1.4954601266269689,\n",
       " 1.8986458118523217,\n",
       " 7.7972793355069152,\n",
       " 2613014.9799408969,\n",
       " 9668.5668750285349,\n",
       " 122.1289147117548,\n",
       " 11.429085154568304)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THREE REGIMES FIT (SPOKEN)\n",
    "sigma =np.ones(len(y_s))\n",
    "sigma[[99, 100, 101, 500, 1700, 1999, 2000, 2001, 2002, -1]] = 0.001\n",
    "(a3S_1, a3S_2,a3S_3,beta_3S, c3S_1, c3S_2, c3S_3, C4_S), _ = opt.curve_fit(zipf_mand_fit_3S, \n",
    "                                                                     x_s, y_s, \n",
    "                                                                     p0=(1., 1.3, 2.5, \n",
    "                                                                     3, \n",
    "                                                                     100000, 1000, 100, 10),\n",
    "                                                                     sigma=sigma)\n",
    "(a3S_1, a3S_2,a3S_3,beta_3S, c3S_1, c3S_2, c3S_3, C4_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98779924555558052,\n",
       " 0.78013700474286507,\n",
       " 1.6496670400187725,\n",
       " 0.60344865923415125,\n",
       " 501780.35693374212,\n",
       " 5544.6062437143801,\n",
       " 279.60203407490104,\n",
       " -259.15257421842836)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3S WRITTEN FIT\n",
    "sigma =np.ones(len(y_w))\n",
    "sigma[[99, 100, 101, 500, 1700, 1999, 2000, 2001, 2002, -1]] = 0.001\n",
    "(a3S_w1, a3S_w2,a3S_w3,beta_3w, c3S_w1, c3S_w2, c3S_w3, C4_w), _ = opt.curve_fit(zipf_mand_fit_3S, \n",
    "                                                            x_w, y_w, \n",
    "                                                            p0=(1.,1.3,2.5, \n",
    "                                                                3, 100000, 1000,100, 10),\n",
    "                                                            sigma=sigma)\n",
    "(a3S_w1, a3S_w2,a3S_w3,beta_3w, c3S_w1, c3S_w2, c3S_w3, C4_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define coefficients for power laws\n",
    "# alpha exponents are no longer a function of age\n",
    "# They are constant and it is vocab size that changes with age\n",
    "\n",
    "\n",
    "# use values obtained with optimization\n",
    "fit_coeffs = {'s':[a3S_1, a3S_2, a3S_3, beta_3S, c3S_1, c3S_2, c3S_3, C4_S], \n",
    "              'w':[a3S_w1, a3S_w2, a3S_w3, beta_3w, c3S_w1, c3S_w2, c3S_w3, C4_w]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00694140842913 216.651070505 4444.17263094\n"
     ]
    }
   ],
   "source": [
    "# find parameters of sigmoid for spoken vocab\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "s_final_size = 20000\n",
    "def equations_s(p):\n",
    "    p1, p2, p3 = p\n",
    "    eqs = ((s_final_size + p3)/(1 + np.exp( -p1 * (0 - p2))) - p3 -1,\n",
    "           (s_final_size + p3)/(1 + np.exp( -p1 * (150 - p2))) - p3 -5000,\n",
    "           (s_final_size + p3)/(1 + np.exp( -p1 * (500 - p2))) - p3 -17000)\n",
    "    return eqs\n",
    "\n",
    "s1,s2,s3 =  fsolve(equations_s, (0.02, 300, 2000))\n",
    "\n",
    "print (s1,s2,s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00458704924706 147.829452321 20300.1841713\n"
     ]
    }
   ],
   "source": [
    "# find parameters of sigmoid for written vocab\n",
    "w_final_size = 40000\n",
    "def equations_w(p):\n",
    "    p1, p2, p3 = p\n",
    "    eqs = ((w_final_size + p3)/(1 + np.exp( -p1 * (0 - p2))) - p3 -2,\n",
    "           (w_final_size + p3)/(1 + np.exp( -p1 * (150 - p2))) - p3 -10000,\n",
    "           (w_final_size + p3)/(1 + np.exp( -p1 * (500 - p2))) - p3 -30000)\n",
    "    return eqs\n",
    "\n",
    "w1,w2,w3 =  fsolve(equations_w, (0.02, 300, 5000))\n",
    "\n",
    "print (w1,w2,w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define age and max vocabulary sizes for both types\n",
    "age = np.linspace(0,3600-1,3600)\n",
    "max_vocab_size = {'s':20000, 'w':40000}\n",
    "\n",
    "# use sigmoid to model vocab size evolution\n",
    "#p1, p2 = 0.02, 260\n",
    "p1, p2,  p3 = s1, s2, s3\n",
    "s_vocab_size_vs_age = (max_vocab_size['s']+p3)/(1 + np.exp( -p1 * (age - p2))) - p3\n",
    "\n",
    "\n",
    "p4, p5, p6 = w1,w2,w3\n",
    "w_vocab_size_vs_age = (max_vocab_size['w'] + p6) /(1 + np.exp( -p4 * (age - p5))) -p6\n",
    "\n",
    "#vocab_size vs age\n",
    "vocab_size = {'s':s_vocab_size_vs_age.astype(np.uint32), \n",
    "              'w':w_vocab_size_vs_age.astype(np.uint32)}\n",
    "\n",
    "def plot_vocab_size_vs_age():\n",
    "    fu,fl = 1.1, 0.9\n",
    "    fig, ax = plt.subplots(2,1)\n",
    "    for i, key in zip([0,1],['s', 'w']):\n",
    "        ax[i].plot(age, fu * vocab_size[key], label = str(round(100*fu,2))+'%')\n",
    "        ax[i].plot(age, vocab_size[key])\n",
    "        ax[i].plot(age, fl * vocab_size[key], label = str(round(100*fl,2))+'%')\n",
    "        ax[i].set_xlabel('age')\n",
    "        ax[i].set_ylabel('vocab_size_in_words')\n",
    "        ax[i].legend(loc='lower right')\n",
    "    \n",
    "    ax[0].set_title('evolution of vocabulary size with age', size=15)\n",
    "    \n",
    "plot_vocab_size_vs_age()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute COMPRESSED CDFs for both 's' and 'w' as a function of age\n",
    "    - Assume max size of vocabs is fixed\n",
    "    - Apply ratio 20000 -> 500 = 40 to all sizes through age values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdfs = defaultdict(list)\n",
    "\n",
    "for lang_type in ['s', 'w']:\n",
    "    for num_words in vocab_size[lang_type]:\n",
    "        cdfs[lang_type].append(Zipf_Mand_3S_CDF_comp(num_words, \n",
    "                                                     *fit_coeffs[lang_type], \n",
    "                                                     n_red=max(int(num_words/40) + 1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now compute FULL CDFS ( not compressed ) for comparison\n",
    "cdfs_FV = defaultdict(list)\n",
    "\n",
    "for lang_type in ['s', 'w']:\n",
    "    for num_words in vocab_size[lang_type]:\n",
    "        cdfs_FV[lang_type].append(Zipf_Mand_3S_CDF(num_words, \n",
    "                                                     *fit_coeffs[lang_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2d9e9e8d0>"
      ]
     },
     "execution_count": 2262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(cdfs['s'][1500],label='s')\n",
    "plt.plot(cdfs['w'][2000],label='w')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SAVE CDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save compressed vocabulary CDFs ( speech and written)\n",
    "# Need to pickle data before saving to hdf5 because deep dish cannot handle object array properly\n",
    "# cdfs is a list of arrays of different shape !!!!!!!!\n",
    "os.chdir('/Users/PG/Paolo/python_repos/language_proj/lang_model_simple/')\n",
    "\n",
    "dd.io.save('cdfs_3R_vs_step.h5', dd.io.ForcePickle(cdfs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/PG/Paolo/python_repos/language_proj/lang_model_simple\")\n",
    "cdfs = dd.io.load('cdfs_3R_vs_step.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE KEY FUNCTION FOR WORD MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yy=[words_day_factor(age) for age in range(3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2da04bb70>]"
      ]
     },
     "execution_count": 2268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MODIF\n",
    "\n",
    "def words_day_factor(age):\n",
    "    \"\"\" Define coeff that determines num hours spoken per day\n",
    "        as pct of vocabulary size, assuming 16000 tokens per adult per day\n",
    "        as average \"\"\"\n",
    "    if age < 36 * 14:\n",
    "        return 2.5 + 100 * np.exp(-0.014 * age)\n",
    "    elif 36 * 14 <= age <= 36 * 65:\n",
    "        return 2.5\n",
    "    elif age > 36 * 65:\n",
    "        return 1.5 + np.exp(0.002 * (age - 36 * 65 ) )\n",
    "\n",
    "# KEY MEMORY FUNCTION \n",
    "# LISTENING MODEL\n",
    "\n",
    "def get_lang_knowledge(t, S, cdf_data, word_counter, num_steps1, num_steps2, \n",
    "                       pct_hours, lang_knowledge,\n",
    "                       vocab_size=40000, a=7.6, b=0.023, c=-0.031, d=-0.2, n_red=1000,\n",
    "                       pct_threshold=0.9, read_hours_per_day=0.5):\n",
    "    \"\"\" Function to compute and update main arrays that define agent linguistic knowledge\n",
    "    \n",
    "        Args:\n",
    "            * t: numpy array(shape=vocab_size) that counts elapsed steps from last activation of each word\n",
    "            * S: numpy array(shape=vocab_size) that measures memory stability for each word\n",
    "            * cdf_data: dict with keys 's','w'. It gives a CDF for each step\n",
    "            * num_steps1, num_steps2: scalars. Initial and final steps for calculation\n",
    "            * pct_hours: percentage of daily hours devoted to chosen language\n",
    "            * lang_knowledge: array with shape = steps. For each step , \n",
    "              it gives percentage knowledge of language as measured \n",
    "              by a given threshold of retrievability(pct_threshold)\n",
    "            * a, b, c, d: parameters to definE memory functioN from SUPERMEMO by Piotr A. Wozniak\n",
    "        \n",
    "        MEMORY MODEL: https://www.supermemo.com/articles/stability.htm\n",
    "        \n",
    "        Assumptions ( see \"HOW MANY WORDS DO WE KNOW ???\" By Marc Brysbaert*, \n",
    "        Michael Stevens, Pawe Mandera and Emmanuel Keuleers): \n",
    "            * ~16000 spoken tokens per day + 16000 heard tokens per day + TV, RADIO\n",
    "            * 1min reading -> 220-300 tokens with large individual differences, thus\n",
    "              in 1 h we get ~ 16000 words\"\"\"\n",
    "    \n",
    "    #initialize R before looping if needed\n",
    "    if not num_steps1:\n",
    "        R = np.zeros(n_red)\n",
    "    else:\n",
    "        R = np.exp(-k * t/S)\n",
    "    \n",
    "    for age in range(num_steps1, num_steps2):\n",
    "\n",
    "        # compute current average COMPRESSED number of words per day\n",
    "        words_per_day = n_red / words_day_factor(age) # pack equation already in function ??\n",
    "        # define correction factor for speech vocab to account for TV, own spoken words, etc...\n",
    "        f_s = 2\n",
    "        if age > 7 * 36 and random.random() > 0.1:\n",
    "            zipf_samples = randZipf(cdf_data['s'][age], int(f_s * pct_hours * words_per_day * 10))\n",
    "            zipf_samples_written = randZipf(cdf_data['w'][age], int(read_hours_per_day * pct_hours * (0.9*words_per_day) * 10))\n",
    "            zipf_samples = np.concatenate((zipf_samples, zipf_samples_written))\n",
    "        else:\n",
    "            zipf_samples = randZipf(cdf_data['s'][age], int(f_s * pct_hours * words_per_day * 10))\n",
    "\n",
    "        # assess which words and how many of each were encountered in current step\n",
    "        act, act_c = np.unique(zipf_samples, return_counts=True)\n",
    "        # update word counter according to previous line\n",
    "        #np.add.at(word_counter, act, act_c)  # \n",
    "        word_counter[act] += act_c\n",
    "        # check which words are available for memorization ( need minimum number of times)\n",
    "        mem_availab_words = np.where(word_counter > 5)[0]\n",
    "        \n",
    "        # compute indices of active words that are available for memory \n",
    "        idxs_act = np.nonzero(np.in1d(act, mem_availab_words, assume_unique=True))\n",
    "        # get really activated words\n",
    "        act = act[idxs_act]\n",
    "        # Apply indices to counts ( retrieve only counts of really active words)\n",
    "        act_c = act_c[idxs_act] \n",
    "\n",
    "\n",
    "        if act.any():  # check that there are any real active words            \n",
    "            # compute increase in memory stability S due to (re)activation\n",
    "            delta_S = a * (S[act]**(-b)) * np.exp(c * 100 * R[act]) + d\n",
    "            # update memory stability value\n",
    "            #np.add.at(S, act, delta_S) # \n",
    "            S[act] += delta_S \n",
    "            \n",
    "            #define mask to update elapsed-steps array t\n",
    "            mask = np.zeros(n_red, dtype=np.bool)\n",
    "            mask[act] = True\n",
    "            t[~mask] += 1 # add ones to last activation time counter if word not act\n",
    "            t[mask] = 0 # set last activation time counter to one if word act\n",
    "            \n",
    "            #discount one to counts\n",
    "            act_c -= 1\n",
    "            #Simplification with good approx : we apply delta_S without iteration !!\n",
    "            delta_S = act_c * (a * (S[act]**(-b)) * np.exp(c * 100 * R[act]) + d)\n",
    "            #np.add.at(S, act, delta_S) # \n",
    "            S[act] += delta_S \n",
    "\n",
    "        else:\n",
    "            #define mask to update elapsed-steps array t\n",
    "            mask = np.zeros(n_red, dtype=np.bool)\n",
    "            mask[act] = True\n",
    "            t[~mask] += 1 # add ones to last activation time counter if word not act\n",
    "            t[mask] = 0 # set last activation time counter to one if word act           \n",
    "            \n",
    "        # memory becomes ever shakier after turning 65...\n",
    "        if age > 65 * 36:\n",
    "            S = np.where(S >= 0.01, S - 0.01, 0.000001)\n",
    "        # compute memory retrievability R from t, S\n",
    "        R = np.exp(-k * t/S)\n",
    "        lang_knowledge[age] = np.where(R > pct_threshold)[0].shape[0] / n_red\n",
    "        \n",
    "    return t, S, lang_knowledge, word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def study_memory_fun(lang_path, cdfs=cdfs, n_red=1000, k = np.log(10/9), rhpd=0.5):\n",
    "    S = np.full(n_red, 0.01)\n",
    "    t = np.full(n_red, 100, dtype=np.int64)\n",
    "    lang_knowledge = np.zeros(3000)\n",
    "    word_counter = np.zeros(n_red, dtype=np.int64)\n",
    "    \n",
    "    for pcts, steps in zip(lang_path['pcts'], zip(lang_path['steps'], lang_path['steps'][1:])):\n",
    "        t, S, lang_knowledge, word_counter =  get_lang_knowledge(t, S, cdfs, word_counter, \n",
    "                                                   *steps, pcts, lang_knowledge, n_red=n_red,\n",
    "                                                   pct_threshold=0.9, read_hours_per_day=rhpd)\n",
    "        \n",
    "    \n",
    "    return t, S, lang_knowledge, word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1833,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = np.log(10/9)\n",
    "lps = [{'pcts':[1,0.9,0.1, 0.4],'steps':[0, 500, 800, 1500, 3000]},\n",
    "       {'pcts':[0,0.1,0.9, 0.6],'steps':[0, 500, 800, 1500, 3000]}]\n",
    "\n",
    "list_t = []\n",
    "list_S = []\n",
    "list_lang_evols = []\n",
    "\n",
    "for lp in lps:\n",
    "    t, S, lang_evol, wc = study_memory_fun(lp)\n",
    "    list_t.append(t)\n",
    "    list_S.append(S)\n",
    "    list_lang_evols.append(lang_evol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1834,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a88d4cf8>"
      ]
     },
     "execution_count": 1834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(list_lang_evols[0],label='pol')\n",
    "plt.plot(list_lang_evols[1],label = 'eng')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lps2 = [{'pcts':[0.5],'steps':[0, 3000]},\n",
    "        {'pcts':[1], 'steps':[0, 3000]}]\n",
    "\n",
    "list_lang_evols2 = []\n",
    "list_t2 = []\n",
    "list_S2 = []\n",
    "for lp in lps2:\n",
    "    x = study_memory_fun(lp)\n",
    "    list_t2.append(x[0])\n",
    "    list_S2.append(x[1])\n",
    "    list_lang_evols2.append(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c7ae4438>"
      ]
     },
     "execution_count": 2317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.plot(list_lang_evols2[0],label= 'fr')\n",
    "# plt.plot(list_lang_evols2[1],label = 'hung')\n",
    "plt.plot(list_lang_evols2[0],label = '50%')\n",
    "plt.plot(list_lang_evols2[1],label = 'cat')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.exp(-k * list_t2[3]/ list_S2[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ba42c7b8>]"
      ]
     },
     "execution_count": 2146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot rolling mean\n",
    "plt.plot(pd.rolling_mean(list_lang_evols2[3], 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generate Initial Conditions for ABM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang_paths_IC = [{'pcts':[0.25],'steps':[0, 1500]},\n",
    "        {'pcts':[0.5],'steps':[0, 1500]},\n",
    "        {'pcts':[0.75],'steps':[0, 1500]},\n",
    "        {'pcts':[1], 'steps':[0, 1500]}]\n",
    "\n",
    "t_IC = []\n",
    "S_IC = []\n",
    "lang_hist = []\n",
    "wc_IC = []\n",
    "for lp in lang_paths_IC:\n",
    "    lang_res = study_memory_fun(lp, rhpd=0.)\n",
    "    t_IC.append(lang_res[0])\n",
    "    S_IC.append(lang_res[1])\n",
    "    lang_hist.append(lang_res[2])\n",
    "    wc_IC.append(lang_res[3])\n",
    "t_IC = np.array(t_IC)\n",
    "S_IC = np.array(S_IC)\n",
    "lang_hist = np.array(lang_hist)\n",
    "wc_IC = np.array(wc_IC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11b26cb70>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(lang_hist[0][:1500])\n",
    "plt.plot(lang_hist[3][:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IC_lang = {}\n",
    "\n",
    "IC_lang['25_pct'] = {'t':t_IC[0],'S':S_IC[0], 'wc':wc_IC[0]}\n",
    "IC_lang['50_pct'] = {'t':t_IC[1],'S':S_IC[1], 'wc':wc_IC[1]}\n",
    "IC_lang['75_pct'] = {'t':t_IC[2],'S':S_IC[2], 'wc':wc_IC[2]}\n",
    "IC_lang['100_pct'] = {'t':t_IC[3],'S':S_IC[3], 'wc':wc_IC[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reading + speaking up to 41 years\n",
    "lang_paths_IC = [{'pcts':[0.25],'steps':[0, 1500]},\n",
    "        {'pcts':[0.5],'steps':[0, 1500]},\n",
    "        {'pcts':[0.75],'steps':[0, 1500]},\n",
    "        {'pcts':[1], 'steps':[0, 1500]}]\n",
    "\n",
    "\n",
    "\n",
    "t_IC = []\n",
    "S_IC = []\n",
    "wc_IC = []\n",
    "for lp in lang_paths_IC:\n",
    "    t_IC.append(study_memory_fun(lp)[0])\n",
    "    S_IC.append(study_memory_fun(lp)[1])\n",
    "    wc_IC.append(study_memory_fun(lp)[3])\n",
    "t_IC = np.array(t_IC)\n",
    "S_IC = np.array(S_IC)\n",
    "wc_IC = np.array(wc_IC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IC_lang = {}\n",
    "\n",
    "# IC_lang['25_pct'] = {'t':t_IC[0],'S':S_IC[0], 'wc':wc_IC[0]}\n",
    "# IC_lang['50_pct'] = {'t':t_IC[1],'S':S_IC[1], 'wc':wc_IC[1]}\n",
    "# IC_lang['75_pct'] = {'t':t_IC[2],'S':S_IC[2], 'wc':wc_IC[2]}\n",
    "# IC_lang['100_pct'] = {'t':t_IC[3],'S':S_IC[3], 'wc':wc_IC[3]}\n",
    "# os.chdir('/Users/PG/Paolo/python_repos/language_proj/lang_model_simple')\n",
    "# dd.io.save('IC_lang_1500_steps.h5', IC_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IC_imp = dd.io.load('IC_lang_1500_steps.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 2400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IC_imp['25_pct']['wc'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMPARISON REDUCED VOCABULARY (RV) VS FULL VOCABULARY(FV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_lang_evols_RV = []\n",
    "lps_RV = [{'pcts':[1, 0.25],'steps':[0, 500, 1000]}]\n",
    "for lp in lps_RV:\n",
    "    list_lang_evols_RV.append(study_memory_fun(lp)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_lang_evols_FV = []\n",
    "lps_FV = [{'pcts':[1, 0.25],'steps':[0, 500, 1000]}]\n",
    "for lp in lps_FV :\n",
    "    list_lang_evols_FV.append(study_memory_fun(lp, cdfs=cdfs_FV, n_red=40000)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b557f3c8>"
      ]
     },
     "execution_count": 1441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(list_lang_evols_RV[0][:1000],label='RV')\n",
    "plt.plot(list_lang_evols_FV[0][:1000],label='FV')\n",
    "plt.legend()\n",
    "\n",
    "# conclusion : exactly same macro-trend, \n",
    "# RV shows more local oscillations due to reduced vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S_1 = np.full(n_red, 0.01)\n",
    "t_1 = np.full(n_red, 100, dtype=np.int64)\n",
    "lang_knowledge_1 = np.zeros(3000)\n",
    "word_counter_1 = np.zeros(n_red, dtype=np.int32)\n",
    "%lprun -f get_lang_knowledge get_lang_knowledge(t_1, S_1, cdf_data_zpf_mand, word_counter_1, 0, 200, 1, lang_knowledge_1, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1252ab6d8>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.plot(lang_knowledge,label=\"z\")\n",
    "plt.plot(lang_knowledge,label=\"base\")\n",
    "# plt.plot(lang_knowledge_zm,label=\"zm\")\n",
    "# plt.plot(lang_knowledge_zm2,label=\"zm2\")\n",
    "# plt.plot(lang_knowledge_zm3,label=\"zm3\")\n",
    "# plt.plot(lang_knowledge_025,label=\"zm4\")\n",
    "# plt.xlabel('steps')\n",
    "# plt.ylabel('% knowledge')\n",
    "# plt.ylim(0,1)\n",
    "# plt.legend()\n",
    "# plt.grid('on')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8785.9887487181149, 8939.8527583142732)"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(S_zm[1:100], weights=1/np.arange(1,100)), np.average(S_zm2[1:100], weights=1/np.arange(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def speak_model():\n",
    "    # SPEAKING MODELING\n",
    "    t_imp = IC_imp['75_pct']['t']\n",
    "    S_imp = IC_imp['75_pct']['S']\n",
    "    R = np.exp( - k * t_imp/S_imp)\n",
    "    f = 1000 / words_day_factor(1500)\n",
    "\n",
    "    # sample must come from AVAILABLE listened words !!!! This can be done in two steps\n",
    "\n",
    "    # 1. First sample from lang CDF ( that encapsulates all needed concepts)\n",
    "    zipf_samples = randZipf(cdf_data_zpf_mand['speech'][1500], int(f * 10))\n",
    "    act, act_c = np.unique(zipf_samples, return_counts=True)\n",
    "    # 2. Then compare to known words\n",
    "    mask_R = np.random.rand(R[act].shape[0]) <= R[l1][act] # get words index from available ones\n",
    "\n",
    "    s_words = act[mask_R]\n",
    "    \n",
    "    #if shapes are different and BOTH agents are bilingual, try to retrieve word from other language\n",
    "    \n",
    "    if s_words.shape != act.shape:\n",
    "        miss_words = R[~mask_R] \n",
    "        R[~mask_R] < R[l2][act]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False, False, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 2334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.random.rand(10) < np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SPEAK PROCESS\n",
    "\n",
    "R2 = np.exp(-k * t_zm2 / S_zm2)\n",
    "\n",
    "#1. Get zipf samples from CDF\n",
    "zipf_cum_distr = cdf_data_zpf_mand['speech'][36*18]\n",
    "numSamples = 50\n",
    "np.unique(randZipf(zipf_cum_distr, numSamples), return_counts=True)\n",
    "\n",
    "np.intersect1d(np.where(R2>0.9), [301,406,806])\n",
    "\n",
    "np.argwhere(u_r_array > 0.9).reshape(51,)\n",
    "\n",
    "u_r_array = np.random.random(R2.shape[0])\n",
    "R2 > u_r_array\n",
    "\n",
    "m = R2[np.where(R2>0.9)].shape[0]\n",
    "u_r_array = np.random.random(m)\n",
    "np.searchsorted(R2[np.where(R2>0.9)], u_r_array)\n",
    "\n",
    "u_r_array < R2[np.where(R2>0.9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_text = \" Bon da; quina calor que fa avuy! nos pot anar. Mri, ara mateix una servidora n'estava parlant; no's pot resistir. Oy, qu'es cert; vamos, estigan bones: me'n vaig a posar a la fresca. Vagi, vagi, estga bona, senyora Marieta. Y encara no han sentit tancar la porta del pis, ja pregunta una ab veu baxa:Qu'ls hi sembla? Dna, lo que deyam; ab tanta roba com porta a sobre, rs li fa goig; tan esllanguida... Si ella es ms prima qu'un fus. Oy tal! sembla un paraygua plegat; tot li baldeja.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word_tokenize(cat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_text = re.sub('[^a-zA-Z]', '', cat_text)\n",
    "#cat_text = re.sub('[\\d]', '', cat_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  Bon da  quina calor que fa avuy  nos pot anar   Mri  ara mateix una servidora n'estava parlant  no's pot resistir   Oy  qu'es cert  vamos  estigan bones  me'n vaig a posar a la fresca   Vagi  vagi  estga bona  senyora Marieta  Y encara no han sentit tancar la porta del pis  ja pregunta una ab veu baxa   Qu'ls hi sembla   Dna  lo que deyam  ab tanta roba com porta a sobre  rs li fa goig  tan esllanguida     Si ella es ms prima qu'un fus   Oy tal  sembla un paraygua plegat  tot li baldeja \""
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_text = re.sub(\"[^\\w']\", ' ', cat_text)\n",
    "cat_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(cat_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(word_tokenize(cat_text)).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_text2 = \"L'endem,  continua la matexa,  qu'era diumenge, axs que ella torn de missa, surto al celobert y pregunto a l'Antonieta si hava vist los estucadors; com aquella tamb es un poch Tana, que sembla Santa Reparada, no'm va entendre fins que li vaig haver signat. Ne volen de rialles?... y com per ass tamb es molt ditxosa,'m fa ab molt desdeny: Ay, filla! no me digas rs que tinch l'home ms cremat qu'una torradora. Y ax? Rs, per que com es tan escrupuls, y ja se sab que tot li fa fstich, diu qu'avuy se'n anir a la fonda axis que senti fregir sardina. Ay, ay! que n'has comprada? N, filla, n; per diu que pel vehinat ha vist que n'enfarinavan!... Y no va poguer dir ms per que'l seu home la va fer ficar dins; com es tan sorrut y de pocus amigus all...,  sin, ja'ls dich que s'hi hagueran pogut llogar cadires.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"L'endem    continua la matexa    qu'era diumenge  axs que ella torn de missa  surto al celobert y pregunto a l'Antonieta si hava vist los estucadors  com aquella tamb es un poch Tana  que sembla Santa Reparada  no'm va entendre fins que li vaig haver signat   Ne volen de rialles     y com per ass tamb es molt ditxosa 'm fa ab molt desdeny    Ay  filla  no me digas rs que tinch l'home ms cremat qu'una torradora     Y ax    Rs  per que com es tan escrupuls  y ja se sab que tot li fa fstich  diu qu'avuy se'n anir a la fonda axis que senti fregir sardina     Ay  ay   que n'has comprada    N  filla  n  per diu que pel vehinat ha vist que n'enfarinavan      Y no va poguer dir ms per que'l seu home la va fer ficar dins  com es tan sorrut y de pocus amigus all      sin  ja'ls dich que s'hi hagueran pogut llogar cadires \""
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_text2 = re.sub(\"[^\\w']\", ' ', cat_text2)\n",
    "cat_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(cat_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(word_tokenize(cat_text2)).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HEAPS LAW ( Type-Token relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "URL = \"https://ca.wikisource.org/wiki/Del_meu_tros_-_La_viuda\"\n",
    "URL2 = \"https://ca.wikisource.org/wiki/Del_meu_tros_-_No_n%27hi_h_d%27altre\"\n",
    "URL3 = \"https://en.wikipedia.org/wiki/1980_Formula_One_season\"\n",
    "html = requests.get(URL).text\n",
    "soup = BeautifulSoup(html, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_from_url(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'xml')\n",
    "    text = [x.text for x in soup.body.find_all('p')]\n",
    "    text = ''.join(text)\n",
    "    text = re.sub(\"[^\\w']\", ' ', text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "class url_data():\n",
    "    \n",
    "    def get_soup(self, url):\n",
    "        html = requests.get(url).text\n",
    "        self.soup = BeautifulSoup(html, 'xml')\n",
    "        \n",
    "    def get_clean_text(self):\n",
    "        self.text = [x.text for x in self.soup.body.find_all('p')]\n",
    "        self.text = ''.join(self.text)\n",
    "        self.text = re.sub(\"[^\\w']\", ' ', self.text)\n",
    "        \n",
    "    def get_tables(self, i):\n",
    "        self.table = self.soup.body.find_all('table')[i]\n",
    "        self.table = pd.read_html(str(self.table.find_all('table')))[0]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_d1 = url_data()\n",
    "url_d1.get_soup(URL3)\n",
    "url_d1.get_tables(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/wiki/Argentina\" title=\"Argentina\"><img alt=\"Argentina\" class=\"thumbborder\" data-file-height=\"500\" data-file-width=\"800\" height=\"14\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/23px-Flag_of_Argentina.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/35px-Flag_of_Argentina.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/46px-Flag_of_Argentina.svg.png 2x\" width=\"23\"/></a>"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_d1.soup.find_all('table')[3].table.span.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_text4 = get_text_from_url(URL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_text3 = [x.text for x in soup.body.find_all('p')]\n",
    "cat_text3 = ''.join(cat_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_text3 = re.sub(\"[^\\w']\", ' ', cat_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2803076923076923"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=6500\n",
    "np.unique(word_tokenize(cat_text3)[:n]).shape[0]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_tokens = word_tokenize(cat_text3)\n",
    "heaps_empiric = [(n, np.unique(text_tokens[:n]).shape[0], np.unique(text_tokens[:n]).shape[0]/n)\n",
    "                 for n in range(50, len(text_tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_text_heaps(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    heaps_empiric = [(n, np.unique(text_tokens[:n]).shape[0], np.unique(text_tokens[:n]).shape[0]/n)\n",
    "                 for n in range(50, len(text_tokens))]\n",
    "    x, y1, y2 =list(zip(*heaps_empiric))\n",
    "    (k,beta), _ = opt.curve_fit(heaps_fun, x, y1, p0=(10,0.5))\n",
    "    ax1 = plt.subplot(211)\n",
    "    ax1.plot(x,y1)\n",
    "    ax1.set_xlabel('num_words')\n",
    "    ax1.set_ylabel('num_types')\n",
    "    ax2 = plt.subplot(212, sharex=ax1 )\n",
    "    ax2.plot(x, y2)\n",
    "    plt.show()\n",
    "    return k, beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.960793571386521, 0.79519369571849796)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_text_heaps(cat_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y1, y2 =list(zip(*heaps_empiric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x108567da0>]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "ax1 = plt.subplot(211)\n",
    "ax1.plot(x,y1)\n",
    "ax1.set_xlabel('num_words')\n",
    "ax1.set_ylabel('num_types')\n",
    "ax2 = plt.subplot(212)\n",
    "ax2.plot(x, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(word_tokenize(cat_text3)[:6500]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10f7840b8>]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_words = np.random.normal(16000,7500,size=100)\n",
    "plt.plot(sp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "def heaps_fun(x, k, beta):\n",
    "    x = np.array(x)\n",
    "    return k * x ** beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(k,beta), _ = opt.curve_fit(heaps_fun, x, y, p0=(10,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7960287211231736, 0.7878744493804708)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10220ec18>]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(x,k*np.array(x)**beta)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1,1000,1000)\n",
    "y = 1/(x + 2.7) ** 1.3\n",
    "plt.plot(x, y)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13d0e5048>]"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(Zipf_Mandelbrot_3S_CDF(40000,fit_coeffs['w'][0],fit_coeffs['w'][1],fit_coeffs['w'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f35692b0>"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for num_w in [100,1000,5000,15000,24000]:\n",
    "    plt.plot(Zipf_Mandelbrot_3S_CDF(num_w),label=str(num_w))\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e9504828>"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=20000\n",
    "plt.plot(Zipf_Mandelbrot_CDF(n,1.32,4.46),label='old')\n",
    "plt.plot(Zipf_Mandelbrot_CDF(n,1.3,8.78),label='ZM')\n",
    "plt.plot(Zipf_Mandelbrot_3S_CDF(n),label='ZM_3S')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16136,)"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(randZipf(Zipf_Mandelbrot_3S_CDF(n),16000*50),return_counts=True)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17362,)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(randZipf(Zipf_Mandelbrot_CDF(n,1.32,4.46),16000*50),return_counts=True)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08138221892381477"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOOKS\n",
    "n_b = 120000\n",
    "alpha_b = 1.05\n",
    "z_cdf = Zipf_CDF(n_b, alpha_b)\n",
    "num_words_sample = 1002504\n",
    "np.unique(randZipf(z_cdf, num_words_sample)).shape[0] / num_words_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05479967948717949"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPEECH\n",
    "n_s = 14700\n",
    "alpha_s = 1.25\n",
    "z_cdf = Zipf_CDF(n_s, alpha_s)\n",
    "num_words_sample = 124800\n",
    "np.unique(randZipf(z_cdf, num_words_sample)).shape[0] / num_words_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=30000\n",
    "alpha=1.05\n",
    "zpf_man_CDF=Zipf_Mandelbrot_CDF(n, alpha)\n",
    "zpf_CDF = Zipf_CDF(n, alpha)\n",
    "zpf_compressed = Zipf_CDF_compressed(n, alpha, n_red=1000)\n",
    "beta = 2.7\n",
    "zpf_mand_compressed = Zipf_Mand_CDF_compressed(n, alpha, beta, n_red=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_real_words =300\n",
    "num_comp_words = num_real_words/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73437999999999992"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.unique(randZipf(zpf_man_CDF, num_real_words)).shape \n",
    "         for _ in range(1000)]) / num_real_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61629333333333336"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.unique(randZipf(zpf_CDF, num_real_words)).shape \n",
    "         for _ in range(1000)]) / num_real_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:51: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.63300000000000001"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.unique(randZipf(zpf_compressed, num_comp_words)).shape \n",
    "         for _ in range(1000)]) / num_comp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:51: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76019999999999999"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.unique(randZipf(zpf_mand_compressed, num_comp_words)).shape \n",
    "         for _ in range(1000)]) / num_comp_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "#matplotlib.use(\"TKAgg\")\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib\n",
    "from matplotlib import animation\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/PG/Paolo/python_repos/language_proj/lang_model_simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[#                             ] | ETA: 00:00:26//anaconda/lib/python3.5/site-packages/matplotlib/tight_layout.py:222: UserWarning: tight_layout : falling back to Agg renderer\n",
      "  warnings.warn(\"tight_layout : falling back to Agg renderer\")\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:03\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "import agent_simple\n",
    "import model_simple\n",
    "reload(agent_simple)\n",
    "reload(model_simple)\n",
    "from model_simple import Simple_Language_Model\n",
    "m1 = Simple_Language_Model(100,width=25, height=25, num_cities=3,\n",
    "                           init_lang_distrib=[0.3, 0.5, 0.2],\n",
    "                           lang_ags_sorted_by_dist=False, lang_ags_sorted_in_clust=False)\n",
    "m1.run_model(100, recording_steps_period=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yy, uu= None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll\n"
     ]
    }
   ],
   "source": [
    "if not yy:\n",
    "    print('ll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
